# Comprehensive V3 Audit Report

**Date**: 2026-01-15
**Auditor**: AI System
**Scope**: `v3/experiments/training/run_training_v2.py` and related components
**Status**: ‚úÖ CRITICAL FIXES APPLIED

---

## Executive Summary

| Category | Issues Found | Fixed | Remaining |
|----------|-------------|-------|-----------|
| **Critical Flaws** | 2 | ‚úÖ 2 | 0 |
| **Design Flaws** | 4 | ‚úÖ 1 | 3 |
| **Data Flaws** | 2 | ‚úÖ 1 | 1 |
| **Minor Issues** | 3 | 0 | 3 |

### Post-Fix Training Results (100 generations, seed=42):
```
Ground Truth Coverage: 100% (72/72 tasks) ‚úÖ
Specialists Emerged: 2/8 (vision, code_math)
Regime Coverage: 40% (2/5 regimes)
Collision Rate: 0%
API Calls: 300
```

---

## üî¥ CRITICAL FLAWS

### Critical Flaw #1: L3 RAG Tool is SIMULATED (Not Real)

**Location**: Lines 749-765 (`_execute_l3`)

```python
async def _execute_l3(self, question: str) -> Tuple[str, float]:
    """L3: RAG (simulated - would need actual document index)."""
    # In production, this would query a vector store
    # For now, we use the LLM to simulate retrieval behavior
    prompt = f"""You are a RAG system. The user is asking about information from documents.
Since you don't have the actual documents, indicate that retrieval is needed.

Question: {question}

Respond with what information you would need to retrieve."""
```

**Problem**:
- L3 RAG is NOT actually querying any documents
- It's just asking the LLM to "pretend" it's a RAG system
- RAG tasks have ground truth answers, but the LLM doesn't have access to the "documents" containing those answers
- This means RAG tasks will almost ALWAYS fail (unless LLM happens to know the answer)

**Impact**:
- RAG specialist emergence is FAKE
- Agents cannot actually learn to use RAG effectively
- RAG ground truth is useless without real document retrieval

**Fix Required**:
```python
# Option 1: Inject context into prompt
async def _execute_l3(self, task: Dict) -> Tuple[str, float]:
    question = task.get('question', '')
    context = task.get('context', '')  # Use the document context!

    prompt = f"""Based on the following document:
{context}

Answer this question: {question}"""

    response = await self.model_l0.generate_content(prompt)
    return response.text, 0.85
```

---

### Critical Flaw #2: Vision Tool Fallback Defeats Purpose

**Location**: Lines 731-747 (`_execute_l2`)

```python
async def _execute_l2(self, task: Dict) -> Tuple[str, float]:
    """L2: Vision."""
    image_path = task.get('image_path')
    question = task.get('question', '')

    if image_path and os.path.exists(image_path):
        # ... use vision ...
    else:
        return await self._execute_l0(question)  # ‚Üê SILENT FALLBACK!
```

**Problem**:
- When vision tasks use fallback images (no real ChartQA), they silently fall back to L0
- The fallback vision tasks have `answer: 'bars'` which L0 can guess
- This creates fake "vision specialist" emergence

**Impact**:
- Vision specialists may not actually be using vision
- Can't distinguish L0 from L2 performance

**Fix Required**:
```python
else:
    # Return low confidence and mark as failed
    return "No image available - cannot use vision", 0.1
```

---

## üü° DESIGN FLAWS

### Design Flaw #1: Fitness Penalty Applied to TASK Regime, Not AGENT Specialty

**Location**: Lines 1035-1037

```python
# FIX #2: Stronger fitness penalty
penalty = compute_strong_fitness_penalty(regime, population, fitness_strength)
adjusted_score = result['confidence'] * penalty
```

**Problem**:
- The penalty is computed based on `regime` (current task's regime)
- But it counts agents specialized in that regime
- An agent who is NOT specialized but WINS in a crowded regime gets penalized
- Should only penalize the WINNER's specialty, not all competitors

**Impact**: Unfair penalty to generalists competing in specialized regimes

**Better Approach**:
```python
# Apply penalty based on WINNER's specialty, not task regime
if winner_agent.specialty:
    penalty = compute_strong_fitness_penalty(winner_agent.specialty, population, fitness_strength)
else:
    penalty = 1.0  # No penalty for generalists
```

---

### Design Flaw #2: Web Tasks Don't Actually Require Web Search

**Location**: Lines 97-146 (TriviaQA fallback)

**Problem**:
- Questions like "Who painted the Mona Lisa?" are trivially answerable by LLM
- L0 can answer most TriviaQA questions correctly
- No real need for web search tool

**Impact**: Web specialists emerge for wrong reasons (not because web search helps)

**Fix Required**: Use RECENT facts or low-popularity queries:
```python
# Questions LLM likely doesn't know (require web search)
{'question': 'What is the population of Bhutan according to 2023 census?',
 'answer': '727145', ...}
{'question': 'Who won the 2025 Super Bowl?',
 'answer': 'TBD', ...}  # Recent events
```

---

### Design Flaw #3: No Tool-Gating Verification

**Problem**: No verification that:
1. L0 fails on tasks meant for other tools
2. Correct tool succeeds where L0 fails
3. There's a meaningful accuracy gap between tools

**Impact**: Can't prove tasks actually require specific tools

**Fix Required**:
```python
def verify_tool_gating():
    """Verify each regime is truly tool-gated."""
    for regime, tasks in task_bank.tasks.items():
        l0_correct = 0
        optimal_correct = 0

        for task in tasks[:5]:  # Sample
            l0_result = execute(task, 'L0')
            optimal_result = execute(task, task['optimal_tool'])

            l0_correct += l0_result['correct']
            optimal_correct += optimal_result['correct']

        gap = optimal_correct - l0_correct
        assert gap >= 2, f"{regime}: Gap too small ({gap}), tasks not tool-gated!"
```

---

### Design Flaw #4: Specialty Threshold Too Low

**Location**: Lines 957-965

```python
def _update_specialty(self):
    if not any(self.wins.values()):
        return

    best_regime = max(self.wins, key=self.wins.get)
    if self.wins[best_regime] >= 3:  # Only 3 wins needed!
        total = sum(self.wins.values())
        if self.wins[best_regime] / max(total, 1) > 0.4:  # Only 40%!
            self.specialty = best_regime
```

**Problem**:
- An agent can specialize with just 3 wins at 40% concentration
- This is very weak - could happen by chance
- Example: 3 wins in vision, 2 wins in code ‚Üí specializes in vision (60%)

**Impact**: Premature/unstable specialization

**Fix Required**:
```python
if self.wins[best_regime] >= 5:  # At least 5 wins
    total = sum(self.wins.values())
    if self.wins[best_regime] / max(total, 1) > 0.5:  # At least 50%
        self.specialty = best_regime
```

---

## üü° DATA FLAWS

### Data Flaw #1: Fallback Vision Tasks Are Trivial

**Location**: Lines 341-347

```python
def _fallback_vision_tasks(self) -> List[Dict]:
    """Fallback if no images available."""
    return [
        {'question': 'Describe what you see in a bar chart showing sales data',
         'answer': 'bars', 'aliases': ['bar chart', 'bar graph'],
         'regime': 'vision', 'optimal_tool': 'L2', 'source': 'fallback'},
    ] * 10  # REPEATED 10 TIMES!
```

**Problems**:
1. Only ONE unique task, repeated 10 times
2. Answer "bars" is guessable without seeing any image
3. No diversity in fallback tasks

**Impact**: Vision regime is meaningless without real images

---

### Data Flaw #2: RAG Context Not Used in Execution

**Location**: The RAG fallback tasks have `context` field but it's never used

```python
{'question': 'What is the API rate limit according to documentation?',
 'answer': '1000', 'aliases': ['1000 requests', '1000 per minute'],
 'context': 'API Documentation: Rate limit is 1000 requests per minute per API key.',  # IGNORED!
 'regime': 'rag', ...}
```

**Problem**: The `context` field containing the "document" is never passed to the L3 executor

**Impact**: RAG tasks are unsolvable since the LLM doesn't have access to the documents

---

## üü¢ MINOR ISSUES

### Minor Issue #1: Hardcoded Confidence Values

**Location**: Lines 729, 745, 765, 780

```python
return text, 0.8   # L1 always 0.8
return text, 0.75  # L2 always 0.75
return text, 0.6   # L3 always 0.6
return answer, 0.85  # L4 always 0.85
```

**Problem**: Confidence doesn't reflect actual answer quality

---

### Minor Issue #2: Memory Boost is Minimal

**Location**: Lines 939-940

```python
if memory_tool and memory_tool in samples:
    samples[memory_tool] *= 1.1  # Only 10% boost
```

**Problem**: 10% boost is too small to meaningfully influence tool selection

---

### Minor Issue #3: No Seed Reproducibility for Tavily

**Location**: Line 775

**Problem**: Tavily web search results are non-deterministic

---

## üìã FIX PRIORITY

| Priority | Issue | Effort | Impact |
|----------|-------|--------|--------|
| **P0** | L3 RAG not using context | 10 min | Critical |
| **P0** | Vision fallback defeats purpose | 10 min | Critical |
| **P1** | Web tasks too easy for LLM | 30 min | High |
| **P1** | Add tool-gating verification | 1 hour | High |
| **P2** | Fitness penalty logic | 20 min | Medium |
| **P2** | Specialty threshold | 5 min | Medium |
| **P3** | Confidence calibration | 30 min | Low |

---

## ‚úÖ RECOMMENDED IMMEDIATE FIXES

### Fix 1: Use RAG Context in L3 Execution

```python
async def _execute_l3(self, task: Dict) -> Tuple[str, float]:
    """L3: RAG - uses document context if available."""
    question = task.get('question', '')
    context = task.get('context', '')

    if context:
        prompt = f"""Based on the following document excerpt:

{context}

Answer this question concisely: {question}"""
    else:
        prompt = f"Answer based on your knowledge: {question}"

    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(
        None, lambda: self.model_l0.generate_content(prompt)
    )
    text = response.text.strip() if response.text else ""
    confidence = 0.85 if context else 0.5
    return text, confidence
```

### Fix 2: Fail Vision Tasks Without Images

```python
async def _execute_l2(self, task: Dict) -> Tuple[str, float]:
    """L2: Vision - requires actual image."""
    image_path = task.get('image_path')
    question = task.get('question', '')

    if image_path and os.path.exists(image_path):
        img = Image.open(image_path)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None, lambda: self.model_l2.generate_content([
                img, f"Look at this image and answer: {question}"
            ])
        )
        text = response.text.strip() if response.text else ""
        return text, 0.85
    else:
        # FAIL - vision required but no image
        return "ERROR: No image provided for vision task", 0.0
```

---

## Summary

### ‚úÖ Fixed Issues:
1. **L3 RAG now uses document context** ‚Üí RAG can answer correctly with context
2. **L2 Vision no longer silently falls back** ‚Üí Returns error if no image
3. **Specialty threshold raised** ‚Üí 5 wins @ 50% concentration (more robust)

### ‚ö†Ô∏è Remaining Issues (Lower Priority):
1. **Web tasks may be too easy** - LLM can answer TriviaQA without search
2. **No tool-gating verification** - Should verify L0 fails where tools succeed
3. **Hardcoded confidence values** - Could be more dynamic
4. **Fallback vision tasks are trivial** - Only meaningful with real ChartQA images

### Post-Fix Observations:
- Fewer specialists emerge (2 vs 6) due to stricter thresholds - this is correct behavior
- Vision and code_math specialize - these have clear tool advantage
- Web/RAG don't specialize - tasks may be solvable without tools

---

*Audit completed: 2026-01-15*
*Fixes applied: 2026-01-15*
